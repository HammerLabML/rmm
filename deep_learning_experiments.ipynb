{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning experiments\n",
    "\n",
    "This notebook accumulates deep learning experiments for all benchmark datasets that can be handled with standard reservoir memory machines, in particular the latch, copy, repeat copy, and signal copy task. We test here a standard gated recurrent unit as well as a gated recurrent unit with a state mechanism like the reservoir memory machine.\n",
    "\n",
    "*Note:* Executing this notebook may take very long, especially for the signal copy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this first cell we set some experimental meta-parameters that are used across all\n",
    "# datasets\n",
    "\n",
    "# the number of training time series\n",
    "N = 90\n",
    "# the number of test time series\n",
    "N_test = 10\n",
    "# the number of repeats for the experiments\n",
    "R = 3\n",
    "# the names of the tasks to be performed\n",
    "tasks = ['latch', 'copy', 'repeat_copy', 'signal_copy']\n",
    "# the number of neurons for each task\n",
    "ms = [64, 256, 256, 64]\n",
    "# the number of input dimensions for each task\n",
    "ns = [1, 9, 9, 2]\n",
    "# the memory size for each task\n",
    "Ks = [2, 21, 11, 2]\n",
    "# the output size for each task\n",
    "Ls = [1, 8, 8, 1]\n",
    "\n",
    "# hyper-parameters for training\n",
    "\n",
    "# the maximum number of epochs\n",
    "num_epochs = 1000\n",
    "# loss threshold for early stopping\n",
    "loss_threshold = 1E-3\n",
    "# minibatch size\n",
    "minibatch_size = 32\n",
    "# the learning rate\n",
    "lr = 1E-3\n",
    "# the weight decay factor\n",
    "weight_decay = 1E-8\n",
    "\n",
    "# variables for error reporting\n",
    "# update factor for moving average over the loss\n",
    "avg_factor = 0.1\n",
    "# number of steps until error is printed\n",
    "print_step = 50\n",
    "\n",
    "# model names\n",
    "models = ['GRU', 'GRU-MM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "After all the hyperparameter setup above we can now iterate over all tasks and\n",
    "first perform hyperparameter optimization, followed by the actual experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Task 4 of 4: signal_copy -----\n",
      "--- repeat 1 of 2 ---\n",
      "-- model: GRU --\n",
      "moving average loss in epoch 50: 4371.52\n",
      "moving average loss in epoch 100: 4537.75\n",
      "moving average loss in epoch 150: 3546.32\n",
      "moving average loss in epoch 200: 4063.7\n",
      "moving average loss in epoch 250: 3967.71\n",
      "moving average loss in epoch 300: 5051.39\n",
      "moving average loss in epoch 350: 5258.5\n",
      "moving average loss in epoch 400: 5569.77\n",
      "moving average loss in epoch 450: 3458.72\n",
      "moving average loss in epoch 500: 6627.35\n",
      "moving average loss in epoch 550: 3168.82\n",
      "moving average loss in epoch 600: 4934.2\n",
      "moving average loss in epoch 650: 5721\n",
      "moving average loss in epoch 700: 3906.8\n",
      "moving average loss in epoch 750: 5287.53\n",
      "moving average loss in epoch 800: 6130.22\n",
      "moving average loss in epoch 850: 6855.02\n",
      "moving average loss in epoch 900: 4567.7\n",
      "moving average loss in epoch 950: 3454.58\n",
      "moving average loss in epoch 1000: 6276.74\n",
      "-- model: GRU-MM --\n",
      "moving average loss in epoch 50: 5077.46\n",
      "moving average loss in epoch 100: 4604.65\n",
      "moving average loss in epoch 150: 5966.96\n",
      "moving average loss in epoch 200: 4296.1\n",
      "moving average loss in epoch 250: 5401.52\n",
      "moving average loss in epoch 300: 6888.35\n",
      "moving average loss in epoch 350: 3054.1\n",
      "moving average loss in epoch 400: 5182.55\n",
      "moving average loss in epoch 450: 6919.77\n",
      "moving average loss in epoch 500: 3330.1\n",
      "moving average loss in epoch 550: 4298.87\n",
      "moving average loss in epoch 600: 4684.01\n",
      "moving average loss in epoch 650: 3972.71\n",
      "moving average loss in epoch 700: 4699.97\n",
      "moving average loss in epoch 750: 4035.34\n",
      "moving average loss in epoch 800: 4779.39\n",
      "moving average loss in epoch 850: 5573.4\n",
      "moving average loss in epoch 900: 5570.57\n",
      "moving average loss in epoch 950: 5865.63\n",
      "moving average loss in epoch 1000: 5320.65\n",
      "--- repeat 2 of 2 ---\n",
      "-- model: GRU --\n",
      "moving average loss in epoch 50: 775.088\n",
      "moving average loss in epoch 100: 750.241\n",
      "moving average loss in epoch 150: 506.707\n",
      "moving average loss in epoch 200: 756.234\n",
      "moving average loss in epoch 250: 863.358\n",
      "moving average loss in epoch 300: 852.331\n",
      "moving average loss in epoch 350: 892.347\n",
      "moving average loss in epoch 400: 648.046\n",
      "moving average loss in epoch 450: 626.368\n",
      "moving average loss in epoch 500: 835.08\n",
      "moving average loss in epoch 550: 495.569\n",
      "moving average loss in epoch 600: 478.379\n",
      "moving average loss in epoch 650: 788.163\n",
      "moving average loss in epoch 700: 854.854\n",
      "moving average loss in epoch 750: 806.127\n",
      "moving average loss in epoch 800: 619.576\n",
      "moving average loss in epoch 850: 629.2\n",
      "moving average loss in epoch 900: 812.05\n",
      "moving average loss in epoch 950: 900.454\n",
      "moving average loss in epoch 1000: 622.081\n",
      "-- model: GRU-MM --\n",
      "moving average loss in epoch 50: 607.264\n",
      "moving average loss in epoch 100: 917.485\n",
      "moving average loss in epoch 150: 588.04\n",
      "moving average loss in epoch 200: 624.9\n",
      "moving average loss in epoch 250: 758.296\n",
      "moving average loss in epoch 300: 857.646\n",
      "moving average loss in epoch 350: 970.436\n",
      "moving average loss in epoch 400: 778.135\n",
      "moving average loss in epoch 450: 847.62\n",
      "moving average loss in epoch 500: 605.998\n",
      "moving average loss in epoch 550: 680.606\n",
      "moving average loss in epoch 600: 953.521\n",
      "moving average loss in epoch 650: 676.032\n",
      "moving average loss in epoch 700: 722.785\n",
      "moving average loss in epoch 750: 781.467\n",
      "moving average loss in epoch 800: 879.484\n",
      "moving average loss in epoch 850: 814.886\n",
      "moving average loss in epoch 900: 454.486\n",
      "moving average loss in epoch 950: 702.173\n",
      "moving average loss in epoch 1000: 752.734\n",
      "GRU: 5.23597 +- 0.989414 (took 13240.5 seconds)\n",
      "GRU-MM: 8.00126 +- 3.72713 (took 33787.1 seconds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import rmm2.deep_memory_machine as dmm\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from dataset_generators import generate_data\n",
    "\n",
    "# iterate over all tasks\n",
    "for task_idx in range(len(tasks)):\n",
    "    task = tasks[task_idx]\n",
    "    print('------ Task %d of %d: %s -----' % (task_idx+1, len(tasks), task))\n",
    "    # load task hyperparameters\n",
    "    m = ms[task_idx]\n",
    "    n = ns[task_idx]\n",
    "    K = Ks[task_idx]\n",
    "    L = Ls[task_idx]\n",
    "    # initialize error and runtime arrays\n",
    "    errors   = np.zeros((len(models), R))\n",
    "    runtimes = np.zeros((len(models), R))\n",
    "    # iterate over all experimental repeats\n",
    "    for r in range(R):\n",
    "        print('--- repeat %d of %d ---' % (r+1, R))\n",
    "        # sample training and test data\n",
    "        Xs, Qs, Ys = generate_data(N, task)\n",
    "        Xs_test, Qs_test, Ys_test = generate_data(N_test, task)\n",
    "        # now iterate over all models\n",
    "        for model_idx in range(len(models)):\n",
    "            model = models[model_idx]\n",
    "            print('-- model: %s --' % model)\n",
    "            # set up the model\n",
    "            start_time = time.time()\n",
    "            if model == 'GRU':\n",
    "                net = dmm.GRUInterface(m, n, L)\n",
    "            elif model == 'GRU-MM':\n",
    "                net = dmm.DeepMemoryMachine(m, n, K, L)\n",
    "            # set up an optimizer\n",
    "            optim = torch.optim.Adam(net.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "            # set up aux variables\n",
    "            loss_avg = None\n",
    "            j = N\n",
    "            # start training\n",
    "            for epoch in range(num_epochs):\n",
    "                optim.zero_grad()\n",
    "                # generate a new permutation of the data\n",
    "                if j + minibatch_size >= N:\n",
    "                    pi = np.random.permutation(N)\n",
    "                    j = 0\n",
    "                # generate minibatch of data\n",
    "                minibatch_loss = torch.zeros(1)\n",
    "                for i in range(minibatch_size):\n",
    "                    X, Q, Y = Xs[pi[j]], Qs[pi[j]], Ys[pi[j]]\n",
    "                    j += 1\n",
    "                    # compute the loss\n",
    "                    if model == 'GRU':\n",
    "                        Ypred = net(X)\n",
    "                        loss  = torch.nn.functional.mse_loss(Ypred, torch.tensor(Y, dtype=torch.float))\n",
    "                    else:\n",
    "                        loss  = net.compute_teacher_forcing_loss(X, Q, Y)\n",
    "                    # add to minibatch\n",
    "                    minibatch_loss = minibatch_loss + loss\n",
    "                # compute gradient\n",
    "                minibatch_loss.backward()\n",
    "                # perform optimization step\n",
    "                optim.step()\n",
    "                # record loss\n",
    "                if loss_avg is None:\n",
    "                    loss_avg = minibatch_loss.item() / minibatch_size\n",
    "                else:\n",
    "                    loss_avg = avg_factor * minibatch_loss.item() / minibatch_size + (1. - avg_factor) * loss_avg\n",
    "                if (epoch + 1) % print_step == 0:\n",
    "                    print('moving average loss in epoch %d: %g' % (epoch+1, loss_avg))\n",
    "                if loss_avg < loss_threshold:\n",
    "                    print('ended training already after %d epochs because moving average loss %g was below loss threshold.' % (epoch + 1, loss_avg))\n",
    "                    break\n",
    "            # measure the RMSE on the test data\n",
    "            mse = 0.\n",
    "            for i in range(N_test):\n",
    "                Ypred = net(Xs_test[i])\n",
    "                mse   += np.mean((Ypred.detach().numpy() - Ys_test[i]) ** 2)\n",
    "            rmse = np.sqrt(mse / N_test)\n",
    "            runtimes[model_idx, r] = time.time() - start_time\n",
    "            errors[model_idx, r] = rmse\n",
    "    # print results\n",
    "    for model_idx in range(len(models)):\n",
    "        print('%s: %g +- %g (took %g seconds)' % (models[model_idx], np.mean(errors[model_idx, :]), np.std(errors[model_idx, :]), np.mean(runtimes[model_idx, :])))\n",
    "    # write results to file\n",
    "    np.savetxt('%s_deep_errors.csv' % task, errors.T, delimiter='\\t', header='\\t'.join(models), comments='')\n",
    "    np.savetxt('%s_deep_runtimes.csv' % task, runtimes.T, delimiter='\\t', header='\\t'.join(models), comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
